# -*- coding: utf-8 -*-
"""avendano1daniel (Apr 24, 2025, 2:47:54â€¯PM)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/model-probing-defense/locations/us-west2/repositories/f5775d6b-cc0d-47c8-9b63-4e2161fdf6e4

# Loading Datasets
"""

# General Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import random
# Sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
# TensorFlow / Keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, Conv1DTranspose, BatchNormalization, MaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau


# Load dataset locally
def load_data_local(file_path):
    # Only read the main 3 features and limit to first 14,000 rows (can be changed)
    use_cols = ["SortedList_First_Element", "SortedList_Second_Element", "Margin Loss"]
    df = pd.read_csv(file_path, usecols=use_cols)
    return df[:14000]

# Define local file paths (update these as needed)
files = {
    "benign": "",
    "hsja": "",
    "nes": "",
    "qeba": "",
    "square": "",
    "surfree": "",
    "boundary": "",
    "synthetic": "",
}

"""# Preprocessing"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Load real benign samples
real_df = load_data_from_gcs(bucket_name, files['benign'])

# First split: 60% training, 40% temp (to be split into val + test)
train_df, temp_df = train_test_split(real_df, test_size=0.4, random_state=42)

# Second split: 50% of remaining (20% of total) for val, 20% for test
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

# Reset indices
train_df = train_df.reset_index(drop=True)
val_df   = val_df.reset_index(drop=True)
test_df  = test_df.reset_index(drop=True)


def extract_features_for_medium_ae(sequence_data, window_size=600, stride=60, chunk_size=100):
    top    = np.array(sequence_data['SortedList_First_Element'])
    second = np.array(sequence_data['SortedList_Second_Element'])
    margin = np.array(sequence_data['Margin Loss'])
    n_chunks = window_size // chunk_size
    seqs = []

    for i in range(0, len(top) - window_size + 1, stride):
        w1 = top[i:i+window_size]
        w2 = second[i:i+window_size]
        w3 = margin[i:i+window_size]

        # Stability using second top logit
        diffs = np.abs(np.diff(w2))
        run = longest = 0
        for v in (diffs < 0.01).astype(int):
            run = run + 1 if v else 0
            longest = max(longest, run)

        var_second = np.var(w2)

        if var_second < 0.0005 or longest > window_size // 3:
            stab = 1.0
        elif var_second < 0.001 or longest > window_size // 5:
            stab = 0.8
        elif var_second < 0.005 or longest > window_size // 10:
            stab = 0.5
        else:
            stab = 0.0

        w4 = np.full(window_size, stab)

        # Sub-window stats for margin loss
        mean_margin_chunks, var_margin_chunks = [], []
        for j in range(n_chunks):
            s, e = j * chunk_size, (j + 1) * chunk_size
            mm, vm = np.mean(w3[s:e]), np.var(w3[s:e])
            mean_margin_chunks.append(np.full(window_size, mm))
            var_margin_chunks.append(np.full(window_size, vm))

        features = [w1, w2, w3, w4] + mean_margin_chunks + var_margin_chunks
        seqs.append(np.column_stack(features))

    return np.array(seqs)

# Extract Features
train_sequences = extract_features_for_medium_ae(train_df)
val_sequences   = extract_features_for_medium_ae(val_df)
test_sequences  = extract_features_for_medium_ae(test_df)

print(f"Train: {train_sequences.shape}")
print(f"Val:   {val_sequences.shape}")
print(f"Test:  {test_sequences.shape}")

# Save train_sequences to CSV
flat_train = train_sequences.reshape(train_sequences.shape[0], -1)
n_features = train_sequences.shape[2]
window_len = train_sequences.shape[1]
columns = [f"F{f}_T{t}" for t in range(window_len) for f in range(n_features)]

train_df_out = pd.DataFrame(flat_train, columns=columns)
train_df_out.to_csv("Train_Features.csv", index=False)
print("Saved train sequences to Train_Features.csv")

"""# Architecture & Training"""

# Architecutre & Training

def build_medium_term_ae(input_shape):

    # Input layer
    inputs = Input(shape=input_shape)

    # Encoder
    x = Conv1D(64, kernel_size=5, padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)

    x = Conv1D(32, kernel_size=7, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)

    x = Conv1D(16, kernel_size=9, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)

    # Bottleneck
    bottleneck = Conv1D(8, kernel_size=11, padding='same', activation='relu')(x)
    bottleneck = BatchNormalization()(bottleneck)

    # Decoder
    x = Conv1DTranspose(16, kernel_size=9, strides=2, padding='same', activation='relu')(bottleneck)
    x = BatchNormalization()(x)

    x = Conv1DTranspose(32, kernel_size=7, strides=2, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)

    x = Conv1DTranspose(64, kernel_size=5, strides=2, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)

    # Output layer
    outputs = Conv1D(input_shape[1], kernel_size=3, padding='same', activation='linear')(x)

    # Create and compile model
    model = Model(inputs, outputs)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

    return model

# Define input shape
input_shape = (train_sequences.shape[1], train_sequences.shape[2])
print(f"Input shape: {input_shape}")

# Build the model
medium_ae = build_medium_term_ae(input_shape)
medium_ae.summary()

# Define callbacks for training
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)

# Train the model
print("Training medium-term autoencoder...")
history = medium_ae.fit(
    train_sequences, train_sequences,
    epochs=50,
    batch_size=32,
    validation_data=(val_sequences, val_sequences),
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Plot training history
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Medium-term AE Training History')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Save the model
medium_ae.save("medium_ae.h5")

"""# Evaluation"""

def calculate_reconstruction_errors(original, reconstructed):
    mse = np.mean((original - reconstructed)**2, axis=(1, 2))
    mae = np.mean(np.abs(original - reconstructed), axis=(1, 2))
    stability = []
    for seq in original:
        top = seq[:, 0]
        diffs = np.abs(np.diff(top))
        run = longest = 0
        for v in (diffs < 0.01).astype(int):
            run = run + 1 if v else 0
            longest = max(longest, run)
        var = np.var(top)
        stability.append(
            1.0 if var < 0.0005 or longest > len(top) // 3 else
            0.8 if var < 0.001 or longest > len(top) // 5 else
            0.5 if var < 0.005 or longest > len(top) // 10 else
            0.0
        )
    return {"mse": mse, "mae": mae, "stability": np.array(stability)}

# thresholds on validation set
test_reconstructed = medium_ae.predict(test_sequences)
test_errors = calculate_reconstruction_errors(test_sequences, test_reconstructed)
thresholds = {m: np.mean(v) + 2.5 * np.std(v) for m, v in test_errors.items()}. # <--- Sensitivity can be customized

for atk in ["surfree", "nes"]: # <--- Can be changed for more or less attacks
    df = load_data_from_gcs(bucket_name, files[atk])
    seqs = extract_features_for_medium_ae(df)
    recon = medium_ae.predict(seqs)
    errs = calculate_reconstruction_errors(seqs, recon)
    flags = {m: errs[m] > thresholds[m] for m in errs}

    combined = flags["mse"] | flags["mae"] | flags["stability"]

    y_true = np.concatenate([np.zeros(len(test_sequences)), np.ones(len(combined))])
    y_pred = np.concatenate([np.zeros(len(test_sequences)), combined.astype(int)])

    print(classification_report(y_true, y_pred, target_names=["Benign", "Attack"], digits=2))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=["Benign", "Attack"], yticklabels=["Benign", "Attack"])
    plt.title(f"{atk.upper()} CM")
    plt.show()

    plt.figure(figsize=(12, 4))
    for i, m in enumerate(["mse", "mae", "stability"], 1):
        plt.subplot(1, 3, i)
        sns.histplot(errs[m], bins=50, kde=True, stat='count', label=atk.upper())
        sns.histplot(test_errors[m], bins=50, kde=True, stat='count', label='Benign')
        plt.axvline(thresholds[m], linestyle='--')
        plt.title(m.upper())
        if i == 1:
            plt.legend()
    plt.tight_layout()
    plt.show()

# Combine errors for plotting
plot_data = []

for feature in ["mse", "mae", "stability"]:
    for val in test_errors[feature]:
        plot_data.append({"Sample": "Benign", "Feature": feature.upper(), "Value": val})
    for val in errs[feature]:
        plot_data.append({"Sample": "Attack", "Feature": feature.upper(), "Value": val})

df_plot = pd.DataFrame(plot_data)

"""# Adversarial Injection Evaluation"""

# Section 1: Benign Stream Preparation
def shuffle_blocks(data, block_size=20):
    blocks = [data[i:i+block_size] for i in range(0, len(data), block_size)]
    random.shuffle(blocks)
    return np.concatenate(blocks, axis=0)

def slight_augment(data, noise_std=0.001):
    return data + np.random.normal(0, noise_std, data.shape)

def randomly_drop_sequences(data, drop_prob=0.05):
    return data[[i for i in range(len(data)) if random.random() > drop_prob]]

# Configuration
attack_list = ["hsja", "qeba", "nes", "square", "surfree", "boundary"]
attack_lengths = [8, 12, 25, 39, 47, 63, 84, 99, 150, 222, 343]
benign_seq_count = 3000
num_attacks_to_inject = 10

# Prepare and augment benign stream
base_benign = extract_features_for_medium_ae(test_df)
extended_benign = np.tile(base_benign, (benign_seq_count // len(base_benign) + 1, 1, 1))[:benign_seq_count]
extended_benign = randomly_drop_sequences(slight_augment(shuffle_blocks(extended_benign)), drop_prob=0.03)
stream = extended_benign.copy()
true_labels = np.zeros(len(stream), dtype=int)

# Section 2: Attack Injection
injected_regions, injected_attacks = [], []
used_attacks, attempts = set(), 0

while len(injected_attacks) < num_attacks_to_inject and attempts < 30:
    if len(used_attacks) == len(attack_list):
        used_attacks.clear()

    atk = random.choice([a for a in attack_list if a not in used_attacks])
    used_attacks.add(atk)
    seq_len = random.choice(attack_lengths)
    attempts += 1

    try:
        df = load_data_from_gcs(bucket_name, files[atk])
        attack_raw = extract_features_for_medium_ae(df)[:seq_len]
        strategy = random.choice(["fragmented", "stretched", "ramped", "hybrid", "spike"])

        if strategy == "fragmented":
            attack_seqs = attack_raw[::2]
        elif strategy == "stretched":
            attack_seqs = np.repeat(attack_raw, 2, axis=0)
        elif strategy == "ramped":
            ramped = attack_raw.copy()
            for i in range(len(ramped)):
                ramped[i] *= 1.0 + (i / len(ramped)) * 0.5
            attack_seqs = ramped
        elif strategy == "hybrid":
            alt_atk = random.choice([a for a in attack_list if a != atk])
            alt_raw = extract_features_for_medium_ae(load_data_from_gcs(bucket_name, files[alt_atk]))[:seq_len]
            attack_seqs = np.concatenate([attack_raw[:seq_len // 2], alt_raw[seq_len // 2:]], axis=0)
        elif strategy == "spike":
            spike = attack_raw[0:1] * 5
            insert_at = len(attack_raw) // 2
            attack_seqs = np.concatenate([attack_raw[:insert_at], spike, attack_raw[insert_at:]], axis=0)

        # Determine spread injection point
        stream_len = len(stream)
        section_size = stream_len // (num_attacks_to_inject + 2)
        section_idx = len(injected_regions) + 1
        start = max(0, section_idx * section_size - 40)
        end = min(stream_len - len(attack_seqs) - 1, section_idx * section_size + 40)
        if end <= start: continue
        insert_idx = random.randint(start, end)

        # Inject attack into stream
        stream = np.concatenate([stream[:insert_idx], attack_seqs, stream[insert_idx:]])
        true_labels = np.concatenate([
            true_labels[:insert_idx],
            np.ones(len(attack_seqs), dtype=int),
            true_labels[insert_idx:]
        ])
        injected_regions.append((insert_idx, insert_idx + len(attack_seqs)))
        injected_attacks.append(f"{atk.upper()}-{strategy} ({len(attack_seqs)})")
    except: continue

# Section 3: AE Prediction + Evaluation
recon = medium_ae.predict(stream, verbose=0)
errors = calculate_reconstruction_errors(stream, recon)
flags = (
    (errors["mse"] > thresholds["mse"]) |
    (errors["mae"] > thresholds["mae"]) |
    (errors["stability"] > thresholds["stability"])
).astype(int)

# Detection Report
print("\nCIFAR-10 Detection Report")
print("-" * 85)
print(f"{'Attack':20s} | {'Range':>15s} | {'Detection':>20s} | {'% Flagged':>10s}")
print("-" * 85)

for (start, end), name in zip(injected_regions, injected_attacks):
    segment_flags = flags[start:end]
    flagged = np.sum(segment_flags)
    percent_flagged = (flagged / (end - start)) * 100
    if flagged:
        first = start + np.where(segment_flags == 1)[0][0]
        last = start + np.where(segment_flags == 1)[0][-1]
        result = f"Detected: {first} - {last}"
    else:
        result = "Not Detected"
    print(f"{name:20s} | {start:4d} - {end:<4d}     | {result:>20s} | {percent_flagged:9.1f}%")
print("-" * 85)

# Section 4: Visualization
plt.figure(figsize=(14, 3))
plt.plot(flags, label="Detection Flags", color='red', linewidth=1)
plt.plot(true_labels, label="Ground Truth", color='black', linestyle='--', linewidth=1)
plt.title("Anomaly Detection - Injected Attack Evaluation")
plt.xlabel("Sequence Index")
plt.ylabel("Flag")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Classification Metrics
print("\nClassification Report")
print(classification_report(true_labels, flags, target_names=["Benign", "Attack"], digits=2))

cm = confusion_matrix(true_labels, flags)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Benign", "Attack"], yticklabels=["Benign", "Attack"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()